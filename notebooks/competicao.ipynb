{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 130329,
          "databundleVersionId": 15642138,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31259,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 0.5 Prepara√ß√£o do Dataset (Google Drive)\n",
        "\n",
        "Nesta vers√£o, o dataset fica armazenado no **Google Drive** do usu√°rio, na mesma pasta\n",
        "onde est√° o notebook. Assim, evitamos depend√™ncia de token/credenciais do Kaggle\n",
        "e reduzimos pontos de falha (como upload e autentica√ß√£o).\n",
        "\n",
        "## Como organizar no Drive (obrigat√≥rio)\n",
        "Coloque o dataset **extra√≠do** (n√£o zip) em uma pasta, por exemplo:\n",
        "\n",
        "MyDrive/Ligia_compviz\n",
        "\n",
        "E dentro da pasta Ligia_compviz estaria:\n",
        "*   competicao.ipynb\n",
        "* ligia-compviz <- (DATA_DIR)\n",
        "\n",
        "‚ö†Ô∏è Observa√ß√£o:\n",
        "- Cada usu√°rio precisa ter o dataset local no pr√≥prio Drive.\n"
      ],
      "metadata": {
        "id": "rtHHGXFunpw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0.5.1 Montar Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "yoxQwJiXnq6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.5.2 Definir o caminho do dataset (DATA_DIR)\n",
        "\n",
        "Defina abaixo a pasta no Drive onde voc√™ colocou o dataset extra√≠do.\n",
        "Recomenda√ß√£o: manter o notebook e a pasta do dataset no mesmo diret√≥rio (organiza√ß√£o do projeto).\n"
      ],
      "metadata": {
        "id": "oWWmZtMfnpuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0.5.2 Defina o caminho base do seu projeto no Drive (ajuste s√≥ essa linha)\n",
        "\n",
        "PROJECT_DIR = \"/content/drive/MyDrive/Ligia_compviz\"  # <- ajuste para sua pasta\n",
        "DATA_DIR = f\"{PROJECT_DIR}/ligia-compviz\"            # <- pasta do dataset extra√≠do\n",
        "\n",
        "print(\"PROJECT_DIR:\", PROJECT_DIR)\n",
        "print(\"DATA_DIR:\", DATA_DIR)\n"
      ],
      "metadata": {
        "id": "yX1uWWvZoSJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.5.3 Sanity Check (estrutura do dataset)\n",
        "\n",
        "Validamos se os arquivos/pastas essenciais existem antes de seguir.\n",
        "Isso evita erros comuns como:\n",
        "- dataset na pasta errada\n",
        "- dataset ainda zipado\n",
        "- estrutura diferente do esperado"
      ],
      "metadata": {
        "id": "8qqSAVd1otoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0.5.3 Sanity check enxuto\n",
        "\n",
        "import os\n",
        "\n",
        "expected = {\n",
        "    \"train.csv\": os.path.join(DATA_DIR, \"train.csv\"),\n",
        "    \"test.csv\": os.path.join(DATA_DIR, \"test.csv\"),\n",
        "    \"NORMAL dir\": os.path.join(DATA_DIR, \"train\", \"train\", \"NORMAL\"),\n",
        "    \"PNEUMONIA dir\": os.path.join(DATA_DIR, \"train\", \"train\", \"PNEUMONIA\"),\n",
        "    \"test_images dir\": os.path.join(DATA_DIR, \"test_images\", \"test_images\"),\n",
        "}\n",
        "\n",
        "for name, path in expected.items():\n",
        "    assert os.path.exists(path), f\"‚ùå N√£o encontrado: {name} -> {path}\"\n",
        "print(\"‚úÖ Estrutura m√≠nima OK.\")\n",
        "\n",
        "# Contagens r√°pidas\n",
        "n_norm = len(os.listdir(expected[\"NORMAL dir\"]))\n",
        "n_pne  = len(os.listdir(expected[\"PNEUMONIA dir\"]))\n",
        "n_test = len(os.listdir(expected[\"test_images dir\"]))\n",
        "\n",
        "print(f\"üìä Contagens: NORMAL={n_norm} | PNEUMONIA={n_pne} | TEST={n_test}\")\n",
        "print(\"‚úÖ DATA_DIR pronto:\", DATA_DIR)\n"
      ],
      "metadata": {
        "id": "IW8bYhkcoxhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setup Experimental\n",
        "\n",
        "Nesta se√ß√£o realizamos a configura√ß√£o inicial do ambiente experimental.\n",
        "\n",
        "O objetivo √©:\n",
        "\n",
        "- Garantir **reprodutibilidade cient√≠fica**\n",
        "- Importar bibliotecas necess√°rias\n",
        "- Definir o dispositivo de execu√ß√£o (CPU/GPU)\n",
        "- Estruturar os caminhos do dataset\n",
        "\n",
        "Seguindo as diretrizes do edital, fixamos a semente aleat√≥ria (seed = 42) para assegurar consist√™ncia nos resultados ao longo das execu√ß√µes.\n",
        "\n",
        "Essa etapa √© essencial para garantir:\n",
        "- Controle experimental\n",
        "- Robustez metodol√≥gica\n",
        "- Integridade cient√≠fica\n"
      ],
      "metadata": {
        "id": "ZVftrZHSnk51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bibliotecas Fundamentais\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "# Deep Learning\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "\n",
        "# M√©tricas\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-15T21:13:56.909106Z",
          "iopub.execute_input": "2026-02-15T21:13:56.909736Z",
          "iopub.status.idle": "2026-02-15T21:13:56.914545Z",
          "shell.execute_reply.started": "2026-02-15T21:13:56.909708Z",
          "shell.execute_reply": "2026-02-15T21:13:56.913757Z"
        },
        "id": "UpMLg2ptnk52"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Reprodutibilidade\n",
        "\n",
        "Para garantir a consist√™ncia dos experimentos, fixamos a semente aleat√≥ria em todas as bibliotecas relevantes.\n",
        "\n",
        "Isso evita varia√ß√µes causadas por:\n",
        "\n",
        "- Inicializa√ß√£o aleat√≥ria dos pesos\n",
        "- Embaralhamento de dados\n",
        "- Opera√ß√µes internas do backend CUDA\n",
        "\n",
        "Essa pr√°tica √© essencial em experimentos cient√≠ficos e ser√° mantida ao longo de todo o projeto.\n"
      ],
      "metadata": {
        "id": "Mj1x3HVunk53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reprodutibilidade\n",
        "SEED = 42\n",
        "\n",
        "def seed_everything(seed=SEED):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(SEED)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-15T21:13:56.916340Z",
          "iopub.execute_input": "2026-02-15T21:13:56.916575Z",
          "iopub.status.idle": "2026-02-15T21:13:56.930480Z",
          "shell.execute_reply.started": "2026-02-15T21:13:56.916554Z",
          "shell.execute_reply": "2026-02-15T21:13:56.929754Z"
        },
        "id": "8UvA3evfnk54"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Configura√ß√£o do Dispositivo\n",
        "\n",
        "O treinamento ser√° realizado utilizando GPU quando dispon√≠vel,\n",
        "de forma a acelerar o processo de otimiza√ß√£o do modelo.\n",
        "\n",
        "Caso GPU n√£o esteja dispon√≠vel, o c√≥digo executar√° automaticamente em CPU.\n"
      ],
      "metadata": {
        "id": "lvQOZQKGnk54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Executando em: {device}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-15T21:13:56.931389Z",
          "iopub.execute_input": "2026-02-15T21:13:56.931685Z",
          "iopub.status.idle": "2026-02-15T21:13:56.940780Z",
          "shell.execute_reply.started": "2026-02-15T21:13:56.931656Z",
          "shell.execute_reply": "2026-02-15T21:13:56.940070Z"
        },
        "id": "0LFAnVuynk54"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Estrutura do Dataset\n",
        "\n",
        "O dataset est√° organizado da seguinte forma:\n",
        "\n",
        "- `test_images/test_images/` ‚Üí imagens do conjunto de teste (sem r√≥tulos)\n",
        "- `train/train/NORMAL/` ‚Üí imagens normais\n",
        "- `train/train/PNEUMONIA/` ‚Üí imagens com pneumonia\n",
        "- `train/train.csv` ‚Üí arquivo auxiliar\n",
        "- `train/test.csv` ‚Üí template de submiss√£o\n",
        "\n",
        "As imagens de treino est√£o separadas por classe em pastas distintas,\n",
        "permitindo a constru√ß√£o manual de um DataFrame estruturado.\n"
      ],
      "metadata": {
        "id": "Rrx7Z540nk54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths do Dataset\n",
        "\n",
        "TEST_IMG_DIR  = os.path.join(DATA_DIR, \"test_images/test_images\")\n",
        "TRAIN_ROOT    = os.path.join(DATA_DIR, \"train\")\n",
        "TRAIN_IMG_DIR = os.path.join(TRAIN_ROOT, \"train\")\n",
        "\n",
        "TRAIN_CSV = os.path.join(DATA_DIR, \"train.csv\")\n",
        "TEST_CSV  = os.path.join(DATA_DIR, \"test.csv\")\n",
        "\n",
        "print(\"Test images:\", TEST_IMG_DIR)\n",
        "print(\"Train images:\", TRAIN_IMG_DIR)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-15T21:13:56.941565Z",
          "iopub.execute_input": "2026-02-15T21:13:56.941742Z",
          "iopub.status.idle": "2026-02-15T21:13:56.953648Z",
          "shell.execute_reply.started": "2026-02-15T21:13:56.941725Z",
          "shell.execute_reply": "2026-02-15T21:13:56.953057Z"
        },
        "id": "tcsWhvHWnk55"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Verifica√ß√£o dos Caminhos (Sanity Check)\n",
        "\n",
        "Antes de iniciar qualquer an√°lise ou treinamento, validamos se os diret√≥rios esperados realmente existem e se cont√™m arquivos.\n",
        "\n",
        "Essa etapa evita erros comuns, como:\n",
        "- caminho incorreto no `/kaggle/input/`\n",
        "- pastas vazias por configura√ß√£o errada do dataset\n",
        "- diverg√™ncias na estrutura de diret√≥rios\n"
      ],
      "metadata": {
        "id": "nR9UUEFVnk55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"TEST_IMG_DIR existe?\", os.path.exists(TEST_IMG_DIR))\n",
        "print(\"TRAIN_IMG_DIR existe?\", os.path.exists(TRAIN_IMG_DIR))\n",
        "\n",
        "print(\"\\nAmostra de arquivos em test_images:\", os.listdir(TEST_IMG_DIR)[:5])\n",
        "print(\"Subpastas dentro de train/train:\", os.listdir(TRAIN_IMG_DIR))\n",
        "\n",
        "# contagem r√°pida\n",
        "print(\"\\nQtd test_images:\", len(os.listdir(TEST_IMG_DIR)))\n",
        "print(\"Qtd pneumonia:\", len(os.listdir(os.path.join(TRAIN_IMG_DIR, \"PNEUMONIA\"))))\n",
        "print(\"Qtd normal:\", len(os.listdir(os.path.join(TRAIN_IMG_DIR, \"NORMAL\"))))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-15T21:13:56.954983Z",
          "iopub.execute_input": "2026-02-15T21:13:56.955313Z",
          "iopub.status.idle": "2026-02-15T21:13:56.978304Z",
          "shell.execute_reply.started": "2026-02-15T21:13:56.955291Z",
          "shell.execute_reply": "2026-02-15T21:13:56.977617Z"
        },
        "id": "9aI_m9vhnk55"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Entendimento do Problema\n",
        "\n",
        "A presente competi√ß√£o consiste na classifica√ß√£o de imagens m√©dicas\n",
        "(Raio-X tor√°cico) quanto √† presen√ßa ou aus√™ncia de pneumonia.\n",
        "\n",
        "Trata-se de um problema de:\n",
        "\n",
        "- Classifica√ß√£o bin√°ria\n",
        "- Dom√≠nio m√©dico\n",
        "- Potencial impacto cl√≠nico relevante\n",
        "\n",
        "As classes s√£o:\n",
        "\n",
        "- 0 ‚Üí Normal\n",
        "- 1 ‚Üí Pneumonia\n",
        "\n",
        "---\n",
        "\n",
        "## 2.1 M√©trica de Avalia√ß√£o\n",
        "\n",
        "A m√©trica oficial √© a **ROC-AUC (Receiver Operating Characteristic - Area Under the Curve)**.\n",
        "\n",
        "Isso implica que:\n",
        "\n",
        "- O modelo deve retornar **probabilidades** da classe positiva (pneumonia).\n",
        "- N√£o devemos enviar r√≥tulos bin√°rios (0 ou 1).\n",
        "- A qualidade do ranking das probabilidades √© mais importante do que um threshold fixo.\n",
        "\n",
        "A ROC-AUC mede a capacidade do modelo de:\n",
        "\n",
        "> Discriminar corretamente entre imagens normais e imagens com pneumonia ao longo de diferentes limiares de decis√£o.\n",
        "\n",
        "---\n",
        "\n",
        "## 2.2 Implica√ß√µes Cl√≠nicas\n",
        "\n",
        "No contexto m√©dico:\n",
        "\n",
        "- Falso Negativo ‚Üí paciente com pneumonia classificado como normal (erro grave).\n",
        "- Falso Positivo ‚Üí paciente saud√°vel classificado como doente (impacto menor, mas relevante).\n",
        "\n",
        "Portanto, al√©m da ROC-AUC, analisaremos:\n",
        "\n",
        "- Recall\n",
        "- Matriz de Confus√£o\n",
        "- Distribui√ß√£o de erros\n",
        "\n",
        "Essas an√°lises ser√£o discutidas posteriormente na se√ß√£o de avalia√ß√£o e interpretabilidade.\n"
      ],
      "metadata": {
        "id": "rdafvw9-nk55"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. An√°lise Explorat√≥ria dos Dados (EDA Visual)\n",
        "\n",
        "Antes de iniciar o treinamento do modelo, realizamos uma an√°lise explorat√≥ria\n",
        "visual das imagens dispon√≠veis.\n",
        "\n",
        "O objetivo desta etapa √©:\n",
        "\n",
        "- Verificar o balanceamento entre as classes\n",
        "- Inspecionar padr√µes visuais caracter√≠sticos\n",
        "- Identificar poss√≠veis varia√ß√µes de resolu√ß√£o\n",
        "- Observar diferen√ßas estruturais entre casos normais e pneumonia\n",
        "\n",
        "Essa etapa √© fundamental para fundamentar decis√µes de:\n",
        "- Pr√©-processamento\n",
        "- Escolha de arquitetura\n",
        "- Estrat√©gias de data augmentation\n"
      ],
      "metadata": {
        "id": "bvzOGC7Jnk55"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Estrutura do Conjunto de Treino\n",
        "\n",
        "O conjunto de treino est√° organizado em subpastas por classe, o que facilita a inspe√ß√£o e a cria√ß√£o de um DataFrame para treinamento posteriormente.\n",
        "\n",
        "- `PNEUMONIA/` ‚Üí classe positiva (1)\n",
        "- `NORMAL/` ‚Üí classe negativa (0)\n",
        "\n",
        "Nesta etapa, definimos os caminhos dessas pastas para facilitar a an√°lise explorat√≥ria e garantir consist√™ncia ao longo do notebook.\n"
      ],
      "metadata": {
        "id": "-hE8UcTrnk55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PNE_DIR = os.path.join(TRAIN_IMG_DIR, \"PNEUMONIA\")\n",
        "NOR_DIR = os.path.join(TRAIN_IMG_DIR, \"NORMAL\")\n",
        "\n",
        "assert os.path.exists(PNE_DIR), \"Pasta PNEUMONIA n√£o encontrada.\"\n",
        "assert os.path.exists(NOR_DIR), \"Pasta NORMAL n√£o encontrada.\"\n",
        "\n",
        "print(\"PNE_DIR:\", PNE_DIR)\n",
        "print(\"NOR_DIR:\", NOR_DIR)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-15T21:13:56.979093Z",
          "iopub.execute_input": "2026-02-15T21:13:56.979332Z",
          "iopub.status.idle": "2026-02-15T21:13:56.984052Z",
          "shell.execute_reply.started": "2026-02-15T21:13:56.979311Z",
          "shell.execute_reply": "2026-02-15T21:13:56.983315Z"
        },
        "id": "7CzEh6zPnk56"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Distribui√ß√£o das Classes\n",
        "\n",
        "Nesta etapa, verificamos a quantidade de imagens dispon√≠veis em cada classe.\n",
        "\n",
        "Essa an√°lise √© importante porque:\n",
        "\n",
        "- Desbalanceamento pode enviesar o treinamento (modelo ‚Äúaprende‚Äù mais a classe majorit√°ria).\n",
        "- M√©tricas como ROC-AUC s√£o mais adequadas do que acur√°cia em cen√°rios desbalanceados.\n",
        "- Pode ser necess√°rio ajustar a fun√ß√£o de perda (ex.: `pos_weight`) ou usar estrat√©gias de valida√ß√£o estratificada.\n",
        "\n",
        "A seguir, calculamos a contagem e a propor√ß√£o de cada classe e exibimos um gr√°fico para facilitar a interpreta√ß√£o.\n"
      ],
      "metadata": {
        "id": "N9YyQnpxnk56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Contagem de imagens por classe a partir das pastas\n",
        "n_pne = len(os.listdir(PNE_DIR))    # classe positiva (pneumonia)\n",
        "n_norm = len(os.listdir(NOR_DIR))  # classe negativa (normal)\n",
        "total = n_pne + n_norm\n",
        "\n",
        "# Exibi√ß√£o das quantidades brutas\n",
        "print(\"Pneumonia:\", n_pne)\n",
        "print(\"Normal:\", n_norm)\n",
        "print(\"Total:\", total)\n",
        "\n",
        "# C√°lculo das propor√ß√µes (√∫til para entender desbalanceamento)\n",
        "p_pne = n_pne / total\n",
        "p_norm = n_norm / total\n",
        "\n",
        "print(f\"Propor√ß√£o Pneumonia: {p_pne:.3f}\")\n",
        "print(f\"Propor√ß√£o Normal: {p_norm:.3f}\")\n",
        "\n",
        "# Plot simples para visualizar rapidamente o desbalanceamento\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.bar([\"PNEUMONIA (1)\", \"NORMAL (0)\"], [n_pne, n_norm])\n",
        "plt.title(\"Distribui√ß√£o das Classes no Treino\")\n",
        "plt.ylabel(\"Quantidade de Imagens\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-15T21:13:56.985025Z",
          "iopub.execute_input": "2026-02-15T21:13:56.985305Z",
          "iopub.status.idle": "2026-02-15T21:13:57.095284Z",
          "shell.execute_reply.started": "2026-02-15T21:13:56.985284Z",
          "shell.execute_reply": "2026-02-15T21:13:57.094543Z"
        },
        "id": "Egu70OrYnk56"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Visualiza√ß√£o de Amostras\n",
        "\n",
        "Nesta etapa, visualizamos algumas imagens aleat√≥rias de cada classe para:\n",
        "\n",
        "- Identificar padr√µes visuais iniciais (ex.: opacidades, regi√µes esbranqui√ßadas)\n",
        "- Entender a variabilidade das imagens (contraste, posi√ß√£o, ru√≠do)\n",
        "- Verificar se existe alguma anomalia (ex.: imagens corrompidas, cortes estranhos)\n",
        "\n",
        "Essa inspe√ß√£o tamb√©m orienta decis√µes futuras sobre pr√©-processamento e data augmentation.\n"
      ],
      "metadata": {
        "id": "NWG1l2rMnk56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_random_images(folder, title, n=6, seed=SEED):\n",
        "\n",
        "    #Mostra 'n' imagens aleat√≥rias de uma pasta.\n",
        "    #- folder: caminho da pasta\n",
        "    #- title: t√≠tulo exibido no plot\n",
        "    #- n: quantidade de imagens\n",
        "    #- seed: para reprodutibilidade da amostra\n",
        "\n",
        "    random.seed(seed)\n",
        "    files = os.listdir(folder)\n",
        "    samples = random.sample(files, k=min(n, len(files)))  # evita erro se n > total\n",
        "\n",
        "    # Cria uma figura com n colunas\n",
        "    plt.figure(figsize=(3*n, 3))\n",
        "    for i, fname in enumerate(samples):\n",
        "        path = os.path.join(folder, fname)\n",
        "\n",
        "        # Abre a imagem (convertendo para RGB para evitar problemas de modo)\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "\n",
        "        # Plot\n",
        "        plt.subplot(1, len(samples), i+1)\n",
        "        plt.imshow(img)\n",
        "        plt.title(title)\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Mostra 6 imagens de pneumonia e 6 de normal\n",
        "show_random_images(PNE_DIR, \"PNEUMONIA\", n=6)\n",
        "show_random_images(NOR_DIR, \"NORMAL\", n=6)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-15T21:13:57.096275Z",
          "iopub.execute_input": "2026-02-15T21:13:57.096558Z",
          "iopub.status.idle": "2026-02-15T21:13:58.765761Z",
          "shell.execute_reply.started": "2026-02-15T21:13:57.096534Z",
          "shell.execute_reply": "2026-02-15T21:13:58.764668Z"
        },
        "id": "tjU4awbFnk56"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Observa√ß√µes Iniciais (Inspe√ß√£o Visual)\n",
        "\n",
        "A inspe√ß√£o visual de amostras aleat√≥rias sugere diferen√ßas consistentes entre as classes:\n",
        "\n",
        "- **PNEUMONIA:** as imagens tendem a apresentar regi√µes mais **esbranqui√ßadas/opacas** no campo pulmonar, compat√≠veis com padr√µes de consolida√ß√£o/infiltra√ß√£o.\n",
        "- **NORMAL:** em geral observa-se maior presen√ßa de √°reas **mais escuras** (maior ‚Äúpreto‚Äù associado ao ar nos pulm√µes), com melhor defini√ß√£o de estruturas internas.\n",
        "\n",
        "Entretanto, foi poss√≠vel observar que **algumas imagens normais podem aparecer levemente mais esbranqui√ßadas**, o que pode ocorrer por fatores de aquisi√ß√£o do exame (ex.: incid√™ncia/posi√ß√£o do raio-X, contraste, exposi√ß√£o). Esse comportamento √© relevante pois pode gerar **falsos positivos**, que no contexto cl√≠nico tendem a ser menos graves do que falsos negativos (caso doente classificado como normal).\n",
        "\n",
        "Al√©m disso, a descri√ß√£o do dataset indica que as imagens foram obtidas de **coortes retrospectivas de pacientes pedi√°tricos (1 a 5 anos) do Guangzhou Women and Children‚Äôs Medical Center**. Isso √© importante porque padr√µes anat√¥micos e a distribui√ß√£o de contraste (propor√ß√£o ‚Äúpreto/branco‚Äù) podem diferir significativamente de exames de adultos, afetando a generaliza√ß√£o do modelo para outras popula√ß√µes. :contentReference[oaicite:1]{index=1}\n",
        "\n",
        "Por fim, tamb√©m notamos que em imagens normais costuma haver:\n",
        "- **melhor visualiza√ß√£o do contorno card√≠aco**;\n",
        "- **ossos mais brancos e n√≠tidos**, o que pode servir como pista visual adicional para o modelo (mas tamb√©m exige cuidado para evitar que o modelo aprenda ‚Äúatalhos‚Äù incorretos).\n"
      ],
      "metadata": {
        "id": "w3eA9hjgnk56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 Resolu√ß√£o e Propor√ß√£o (Aspect Ratio)\n",
        "\n",
        "Nesta etapa analisamos a resolu√ß√£o (largura x altura) e a propor√ß√£o (W/H) das imagens do treino.\n",
        "\n",
        "Motiva√ß√µes:\n",
        "\n",
        "- Definir um tamanho padr√£o de entrada (`IMG_SIZE`) para modelos pr√©-treinados.\n",
        "- Identificar varia√ß√µes de resolu√ß√£o que podem afetar o pr√©-processamento.\n",
        "- Avaliar se o `Resize` para um tamanho fixo pode introduzir distor√ß√µes relevantes.\n",
        "\n",
        "Para tornar a an√°lise r√°pida, usamos uma **amostra** de imagens ao inv√©s de carregar todo o conjunto.\n"
      ],
      "metadata": {
        "id": "m_OhgeyUnk56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configura√ß√µes da amostragem\n",
        "sample_size = 300\n",
        "random.seed(SEED)\n",
        "\n",
        "# Lista de caminhos das imagens (misturando as duas classes)\n",
        "pne_files = [os.path.join(PNE_DIR, f) for f in os.listdir(PNE_DIR)]\n",
        "nor_files = [os.path.join(NOR_DIR, f) for f in os.listdir(NOR_DIR)]\n",
        "all_train_files = pne_files + nor_files\n",
        "\n",
        "# Amostra aleat√≥ria para acelerar (sem precisar abrir todas as imagens)\n",
        "sample_paths = random.sample(all_train_files, k=min(sample_size, len(all_train_files)))\n",
        "\n",
        "# Coleta de dimens√µes\n",
        "widths, heights, ratios = [], [], []\n",
        "\n",
        "for path in sample_paths:\n",
        "    # Abre imagem somente para ler tamanho (leve)\n",
        "    with Image.open(path) as img:\n",
        "        w, h = img.size\n",
        "    widths.append(w)\n",
        "    heights.append(h)\n",
        "    ratios.append(w / h)\n",
        "\n",
        "widths = np.array(widths)\n",
        "heights = np.array(heights)\n",
        "ratios = np.array(ratios)\n",
        "\n",
        "# Estat√≠sticas descritivas\n",
        "print(\"Amostra analisada:\", len(sample_paths))\n",
        "print(f\"Largura  - min/mediana/m√°x: {widths.min()} / {int(np.median(widths))} / {widths.max()}\")\n",
        "print(f\"Altura   - min/mediana/m√°x: {heights.min()} / {int(np.median(heights))} / {heights.max()}\")\n",
        "print(f\"Aspect   - min/mediana/m√°x: {ratios.min():.3f} / {np.median(ratios):.3f} / {ratios.max():.3f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-15T21:13:58.767919Z",
          "iopub.execute_input": "2026-02-15T21:13:58.768167Z",
          "iopub.status.idle": "2026-02-15T21:13:59.241149Z",
          "shell.execute_reply.started": "2026-02-15T21:13:58.768145Z",
          "shell.execute_reply": "2026-02-15T21:13:59.240338Z"
        },
        "id": "z-RH0XBbnk56"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Histograma de larguras\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.hist(widths, bins=20)\n",
        "plt.title(\"Distribui√ß√£o de Largura (amostra)\")\n",
        "plt.xlabel(\"Largura (pixels)\")\n",
        "plt.ylabel(\"Frequ√™ncia\")\n",
        "plt.show()\n",
        "\n",
        "# Histograma de alturas\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.hist(heights, bins=20)\n",
        "plt.title(\"Distribui√ß√£o de Altura (amostra)\")\n",
        "plt.xlabel(\"Altura (pixels)\")\n",
        "plt.ylabel(\"Frequ√™ncia\")\n",
        "plt.show()\n",
        "\n",
        "# Histograma do aspect ratio (W/H)\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.hist(ratios, bins=20)\n",
        "plt.title(\"Distribui√ß√£o do Aspect Ratio (W/H) - amostra\")\n",
        "plt.xlabel(\"W/H\")\n",
        "plt.ylabel(\"Frequ√™ncia\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-15T21:13:59.242145Z",
          "iopub.execute_input": "2026-02-15T21:13:59.242423Z",
          "iopub.status.idle": "2026-02-15T21:13:59.591864Z",
          "shell.execute_reply.started": "2026-02-15T21:13:59.242397Z",
          "shell.execute_reply": "2026-02-15T21:13:59.591205Z"
        },
        "id": "GxrFbj8rnk56"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpreta√ß√£o\n",
        "\n",
        "Observamos varia√ß√£o relevante na resolu√ß√£o das imagens (largura e altura), o que exige padroniza√ß√£o antes do treinamento.\n",
        "A maioria das imagens apresenta aspect ratio entre ~1.25 e 1.75, com alguns outliers mais extremos (at√© ~2.7), indicando que muitas imagens s√£o mais largas do que altas.\n",
        "\n",
        "Para utilizar modelos pr√©-treinados, √© necess√°rio definir um tamanho de entrada fixo. Inicialmente, adotaremos `Resize` para um tamanho padr√£o (ex.: 224√ó224) como baseline por simplicidade e reprodutibilidade. Em etapas posteriores, poderemos avaliar estrat√©gias que preservem melhor a propor√ß√£o geom√©trica, como padding (letterbox), caso a distor√ß√£o afete a performance.\n"
      ],
      "metadata": {
        "id": "kCO3n7pNnk57"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Constru√ß√£o do Dataset e Estrat√©gia de Valida√ß√£o\n",
        "\n",
        "Nesta se√ß√£o constru√≠mos os DataFrames de treino e teste de forma consistente com os arquivos oficiais da competi√ß√£o:\n",
        "\n",
        "- `train.csv` cont√©m a refer√™ncia oficial (`id`, `label`) para treinamento.\n",
        "- `test.csv` cont√©m os `id`s esperados no arquivo de submiss√£o.\n",
        "\n",
        "Al√©m disso:\n",
        "- validamos se os arquivos existentes nas pastas correspondem aos `id`s dos CSVs;\n",
        "- montamos o caminho completo de cada imagem (`path`);\n",
        "- definimos uma estrat√©gia de valida√ß√£o com **StratifiedKFold**, preservando o desbalanceamento de classes em cada fold.\n"
      ],
      "metadata": {
        "id": "iwmgMomZnk57"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Carregamento dos CSVs Oficiais\n",
        "\n",
        "Os CSVs fornecidos pela competi√ß√£o s√£o a fonte oficial de:\n",
        "\n",
        "- `id`: identificador do arquivo (inclui extens√£o `.jpeg`)\n",
        "- `label`: r√≥tulo bin√°rio no conjunto de treino\n",
        "\n",
        "Nesta etapa, carregamos os CSVs e inspecionamos seu formato.\n"
      ],
      "metadata": {
        "id": "rK96Gq_Knk57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.1 Leitura dos CSVs\n",
        "\n",
        "train_csv = pd.read_csv(TRAIN_CSV)\n",
        "test_csv  = pd.read_csv(TEST_CSV)\n",
        "\n",
        "display(train_csv.head())\n",
        "display(test_csv.head())\n",
        "\n",
        "print(\"Colunas train.csv:\", train_csv.columns.tolist())\n",
        "print(\"Colunas test.csv:\", test_csv.columns.tolist())\n",
        "print(\"Shapes:\", train_csv.shape, test_csv.shape)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-15T21:13:59.592685Z",
          "iopub.execute_input": "2026-02-15T21:13:59.592973Z",
          "iopub.status.idle": "2026-02-15T21:13:59.613690Z",
          "shell.execute_reply.started": "2026-02-15T21:13:59.592944Z",
          "shell.execute_reply": "2026-02-15T21:13:59.612909Z"
        },
        "id": "qjC-L1-unk57"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Constru√ß√£o do DataFrame de Treino (`df_train`)\n",
        "\n",
        "Constru√≠mos `df_train` a partir do `train.csv`, garantindo que:\n",
        "\n",
        "- cada `id` tenha um arquivo correspondente na pasta de imagens;\n",
        "- o `label` utilizado √© o r√≥tulo oficial fornecido pela competi√ß√£o.\n",
        "\n",
        "Tamb√©m criamos a coluna `path` com o caminho completo da imagem para uso no DataLoader.\n"
      ],
      "metadata": {
        "id": "gfcnVbsKnk57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.2 df_train: CSV -> path\n",
        "\n",
        "# Conjunto de arquivos existentes nas pastas (ids com extens√£o)\n",
        "files_pne = set(os.listdir(PNE_DIR))\n",
        "files_nor = set(os.listdir(NOR_DIR))\n",
        "files_all = files_pne | files_nor\n",
        "\n",
        "# Checagem: todo id do train.csv precisa existir nas pastas\n",
        "missing_train_files = set(train_csv[\"id\"]) - files_all\n",
        "print(\"Arquivos do train.csv que N√ÉO est√£o nas pastas:\", len(missing_train_files))\n",
        "assert len(missing_train_files) == 0, \"Existem ids no train.csv sem arquivo correspondente nas pastas.\"\n",
        "\n",
        "# Fun√ß√£o para criar o path correto de cada id\n",
        "def build_train_path(img_id: str) -> str:\n",
        "    # Decide se o arquivo est√° em PNE_DIR ou NOR_DIR\n",
        "    if img_id in files_pne:\n",
        "        return os.path.join(PNE_DIR, img_id)\n",
        "    elif img_id in files_nor:\n",
        "        return os.path.join(NOR_DIR, img_id)\n",
        "    else:\n",
        "        # Isso n√£o deveria acontecer por causa do assert acima\n",
        "        return None\n",
        "\n",
        "df_train = train_csv.copy()\n",
        "df_train[\"path\"] = df_train[\"id\"].apply(build_train_path)\n",
        "\n",
        "# Checagem final: nenhum path pode ficar nulo\n",
        "assert df_train[\"path\"].isna().sum() == 0, \"Alguns paths ficaram nulos. Verifique as pastas.\"\n",
        "\n",
        "# Embaralha para n√£o ficar com blocos ordenados\n",
        "df_train = df_train.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
        "\n",
        "print(\"df_train shape:\", df_train.shape)\n",
        "df_train.head()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-15T21:13:59.614577Z",
          "iopub.execute_input": "2026-02-15T21:13:59.614805Z",
          "iopub.status.idle": "2026-02-15T21:13:59.639745Z",
          "shell.execute_reply.started": "2026-02-15T21:13:59.614784Z",
          "shell.execute_reply": "2026-02-15T21:13:59.639067Z"
        },
        "id": "u85KCfBfnk57"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Constru√ß√£o do DataFrame de Teste (`df_test`)\n",
        "\n",
        "No conjunto de teste, os r√≥tulos n√£o s√£o fornecidos. O objetivo √©:\n",
        "\n",
        "- associar cada `id` do `test.csv` ao respectivo arquivo na pasta `test_images`;\n",
        "- garantir que os `ids` utilizados na submiss√£o sigam exatamente o formato esperado (incluindo `.jpeg`).\n"
      ],
      "metadata": {
        "id": "4m6kGseJnk57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.3 df_test: test.csv -> path\n",
        "\n",
        "# Arquivos dispon√≠veis na pasta de teste\n",
        "test_files_in_dir = set(os.listdir(TEST_IMG_DIR))\n",
        "\n",
        "# Checagem: todo id do test.csv precisa existir na pasta\n",
        "missing_test_files = set(test_csv[\"id\"]) - test_files_in_dir\n",
        "print(\"Arquivos do test.csv que N√ÉO est√£o na pasta:\", len(missing_test_files))\n",
        "assert len(missing_test_files) == 0, \"Existem ids no test.csv sem arquivo correspondente em test_images.\"\n",
        "\n",
        "df_test = test_csv.copy()  # mant√©m o id oficial\n",
        "df_test[\"path\"] = df_test[\"id\"].apply(lambda x: os.path.join(TEST_IMG_DIR, x))\n",
        "\n",
        "print(\"df_test shape:\", df_test.shape)\n",
        "df_test.head()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-15T21:13:59.640668Z",
          "iopub.execute_input": "2026-02-15T21:13:59.640948Z",
          "iopub.status.idle": "2026-02-15T21:13:59.652920Z",
          "shell.execute_reply.started": "2026-02-15T21:13:59.640918Z",
          "shell.execute_reply": "2026-02-15T21:13:59.652138Z"
        },
        "id": "UHcOfBtpnk57"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 Valida√ß√£o com StratifiedKFold\n",
        "\n",
        "Como o dataset √© desbalanceado, utilizamos **StratifiedKFold** para que cada fold mantenha uma propor√ß√£o de classes semelhante ao conjunto original.\n",
        "\n",
        "Isso melhora a confiabilidade da valida√ß√£o e reduz varia√ß√µes artificiais entre folds.\n"
      ],
      "metadata": {
        "id": "der7iQOsnk57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.4 Cria√ß√£o dos folds\n",
        "\n",
        "N_SPLITS = 5\n",
        "\n",
        "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
        "\n",
        "df_train[\"fold\"] = -1  # coluna para guardar o fold de cada amostra\n",
        "\n",
        "for fold, (_, val_idx) in enumerate(skf.split(df_train[\"path\"], df_train[\"label\"])):\n",
        "    df_train.loc[val_idx, \"fold\"] = fold\n",
        "\n",
        "# Checagem: distribui√ß√£o de classes por fold (deve ser semelhante)\n",
        "for f in range(N_SPLITS):\n",
        "    counts = df_train[df_train[\"fold\"] == f][\"label\"].value_counts(normalize=True)\n",
        "    print(f\"Fold {f} propor√ß√µes:\\n{counts}\\n\")\n",
        "\n",
        "df_train.head()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-15T21:13:59.653753Z",
          "iopub.execute_input": "2026-02-15T21:13:59.654001Z",
          "iopub.status.idle": "2026-02-15T21:13:59.677284Z",
          "shell.execute_reply.started": "2026-02-15T21:13:59.653979Z",
          "shell.execute_reply": "2026-02-15T21:13:59.676732Z"
        },
        "id": "FZ7-so8xnk57"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Pr√©-processamento e DataLoaders\n",
        "\n",
        "Nesta se√ß√£o definimos como as imagens ser√£o preparadas antes de entrarem no modelo.\n",
        "\n",
        "Inclui:\n",
        "\n",
        "- Defini√ß√£o do tamanho padr√£o da imagem (`IMG_SIZE`)\n",
        "- Transforma√ß√µes para treino (com data augmentation controlado)\n",
        "- Transforma√ß√µes para valida√ß√£o/teste (sem augmentation)\n",
        "- Cria√ß√£o de um `Dataset` customizado e `DataLoader`\n",
        "\n",
        "Como utilizaremos modelos pr√©-treinados, aplicamos normaliza√ß√£o compat√≠vel com ImageNet.\n",
        "Al√©m disso, aplicamos augmentation apenas no treino para melhorar generaliza√ß√£o,\n",
        "mantendo valida√ß√£o/teste determin√≠sticos.\n"
      ],
      "metadata": {
        "id": "XNwQvDHlnk57"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 Transforma√ß√µes (Transforms)\n",
        "\n",
        "Como vimos no EDA, as imagens possuem resolu√ß√µes e aspect ratios variados.\n",
        "Para alimentar redes neurais, padronizamos o tamanho.\n",
        "\n",
        "- **Treino:** inclui augmentations leves (ex.: rota√ß√£o pequena e flip horizontal),\n",
        "  visando robustez a varia√ß√µes de aquisi√ß√£o.\n",
        "- **Valida√ß√£o/Teste:** apenas resize + normaliza√ß√£o, para medir desempenho de forma est√°vel.\n"
      ],
      "metadata": {
        "id": "xlBYRBq6nk57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 224  # baseline comum em modelos pr√©-treinados (ResNet/EfficientNet)\n",
        "\n",
        "# Transforma√ß√µes do treino (com augmentation leve)\n",
        "train_tfms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),        # padroniza o tamanho\n",
        "    transforms.RandomHorizontalFlip(p=0.5),          # varia√ß√£o leve (n√£o altera anatomia verticalmente)\n",
        "    transforms.RandomRotation(degrees=10),           # rota√ß√£o pequena (simula varia√ß√£o de posicionamento)\n",
        "    transforms.ToTensor(),                          # converte para tensor [0,1]\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],      # normaliza√ß√£o ImageNet\n",
        "                         [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Transforma√ß√µes para valida√ß√£o e teste (sem augmentation)\n",
        "valid_tfms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225]),\n",
        "])\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-15T21:13:59.678206Z",
          "iopub.execute_input": "2026-02-15T21:13:59.678632Z",
          "iopub.status.idle": "2026-02-15T21:13:59.684874Z",
          "shell.execute_reply.started": "2026-02-15T21:13:59.678600Z",
          "shell.execute_reply": "2026-02-15T21:13:59.684167Z"
        },
        "id": "xFFyj0-Wnk57"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 Dataset Customizado\n",
        "\n",
        "Criamos um Dataset customizado para:\n",
        "\n",
        "- Ler imagens a partir da coluna `path`\n",
        "- Retornar `(imagem, label)` no treino/valida√ß√£o\n",
        "- Retornar apenas `imagem` no teste\n",
        "\n",
        "A convers√£o para `RGB` √© utilizada para compatibilidade direta com modelos pr√©-treinados em ImageNet.\n"
      ],
      "metadata": {
        "id": "kd2rnWUfnk58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class XRayDataset(Dataset):\n",
        "    def __init__(self, df, transform=None, has_label=True):\n",
        "\n",
        "        # df: DataFrame com colunas:\n",
        "        # - path (obrigat√≥rio)\n",
        "        # - label (se has_label=True)\n",
        "\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "        self.has_label = has_label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "\n",
        "        # Abre imagem e converte para RGB (compat√≠vel com modelos ImageNet)\n",
        "        img = Image.open(row[\"path\"]).convert(\"RGB\")\n",
        "\n",
        "        # Aplica transforma√ß√µes\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        # Se houver label (treino/valida√ß√£o), retorna (img, label)\n",
        "        if self.has_label:\n",
        "            y = torch.tensor(row[\"label\"], dtype=torch.float32)\n",
        "            return img, y\n",
        "\n",
        "        # Caso teste, retorna apenas imagem\n",
        "        return img\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-15T21:13:59.685954Z",
          "iopub.execute_input": "2026-02-15T21:13:59.686277Z",
          "iopub.status.idle": "2026-02-15T21:13:59.697268Z",
          "shell.execute_reply.started": "2026-02-15T21:13:59.686247Z",
          "shell.execute_reply": "2026-02-15T21:13:59.696573Z"
        },
        "id": "0UzYvPXQnk58"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3 DataLoaders com Separa√ß√£o por Fold\n",
        "\n",
        "Usaremos a coluna `fold` criada na Se√ß√£o 4 para separar treino e valida√ß√£o.\n",
        "\n",
        "Isso garante que:\n",
        "- treino e valida√ß√£o s√£o disjuntos\n",
        "- a valida√ß√£o preserva a distribui√ß√£o de classes (stratified)\n"
      ],
      "metadata": {
        "id": "NVXPOq8hnk58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_loaders(df_train, fold=0, batch_size=32, num_workers=2):\n",
        "    # Separa treino e valida√ß√£o\n",
        "    df_tr = df_train[df_train[\"fold\"] != fold].reset_index(drop=True)\n",
        "    df_va = df_train[df_train[\"fold\"] == fold].reset_index(drop=True)\n",
        "\n",
        "    # Cria datasets\n",
        "    ds_tr = XRayDataset(df_tr, transform=train_tfms, has_label=True)\n",
        "    ds_va = XRayDataset(df_va, transform=valid_tfms, has_label=True)\n",
        "\n",
        "    # DataLoaders\n",
        "    dl_tr = DataLoader(ds_tr, batch_size=batch_size, shuffle=True,\n",
        "                       num_workers=num_workers, pin_memory=True)\n",
        "    dl_va = DataLoader(ds_va, batch_size=batch_size, shuffle=False,\n",
        "                       num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "    return dl_tr, dl_va\n",
        "\n",
        "# Exemplo: loaders do fold 0\n",
        "train_loader, valid_loader = make_loaders(df_train, fold=0, batch_size=32)\n",
        "\n",
        "# Test loader\n",
        "test_dataset = XRayDataset(df_test, transform=valid_tfms, has_label=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(\"Train batches:\", len(train_loader))\n",
        "print(\"Valid batches:\", len(valid_loader))\n",
        "print(\"Test batches :\", len(test_loader))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-15T21:13:59.698027Z",
          "iopub.execute_input": "2026-02-15T21:13:59.698274Z",
          "iopub.status.idle": "2026-02-15T21:13:59.711617Z",
          "shell.execute_reply.started": "2026-02-15T21:13:59.698254Z",
          "shell.execute_reply": "2026-02-15T21:13:59.710865Z"
        },
        "id": "PLosiXdBnk58"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Modelagem e Treinamento\n",
        "\n",
        "Nesta se√ß√£o definimos e treinamos um modelo de Deep Learning para classificar\n",
        "imagens de raio-X tor√°cico em:\n",
        "\n",
        "- 0 ‚Üí Normal\n",
        "- 1 ‚Üí Pneumonia\n",
        "\n",
        "Como estrat√©gia principal, utilizamos **Transfer Learning** com uma rede pr√©-treinada em ImageNet.\n",
        "Isso √© √∫til pois o dataset n√£o √© gigantesco e o modelo pr√©-treinado j√° possui filtros visuais\n",
        "√∫teis (bordas, texturas, padr√µes), acelerando a converg√™ncia.\n",
        "\n",
        "A sa√≠da do modelo ser√° um **score cont√≠nuo** (logit), que ser√° convertido em probabilidade via sigmoide.\n",
        "A m√©trica de valida√ß√£o ser√° a **ROC-AUC**, compat√≠vel com a avalia√ß√£o da competi√ß√£o.\n"
      ],
      "metadata": {
        "id": "o1fO1-m6nk6F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1 Arquitetura do Modelo\n",
        "\n",
        "Usaremos a EfficientNet-B0 pr√©-treinada. Substitu√≠mos a camada final para produzir 1 logit:\n",
        "\n",
        "- logit ‚Üí `sigmoid(logit)` = probabilidade de pneumonia\n",
        "\n",
        "Essa configura√ß√£o √© adequada para classifica√ß√£o bin√°ria com `BCEWithLogitsLoss`,\n",
        "que aplica internamente a sigmoide de forma numericamente est√°vel.\n"
      ],
      "metadata": {
        "id": "kUT5sbGtnk6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "\n",
        "def build_model():\n",
        "    # Carrega EfficientNet-B0 pr√©-treinada\n",
        "    model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
        "\n",
        "    # Troca a camada final para sa√≠da bin√°ria (1 logit)\n",
        "    in_features = model.classifier[1].in_features\n",
        "    model.classifier[1] = nn.Linear(in_features, 1)\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-15T21:13:59.712698Z",
          "iopub.execute_input": "2026-02-15T21:13:59.712937Z",
          "iopub.status.idle": "2026-02-15T21:13:59.722742Z",
          "shell.execute_reply.started": "2026-02-15T21:13:59.712915Z",
          "shell.execute_reply": "2026-02-15T21:13:59.722175Z"
        },
        "id": "C8FZQFo1nk6F"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2 Fun√ß√£o de Perda e Otimizador\n",
        "\n",
        "Como o dataset √© desbalanceado (maioria pneumonia), utilizamos `pos_weight` na loss\n",
        "para penalizar mais erros na classe positiva (pneumonia).\n",
        "\n",
        "A loss escolhida √©:\n",
        "\n",
        "- `BCEWithLogitsLoss(pos_weight=...)`\n",
        "\n",
        "Otimizador:\n",
        "- `AdamW` (boa estabilidade e regulariza√ß√£o via weight decay)\n",
        "\n",
        "Scheduler:\n",
        "- `CosineAnnealingLR` (reduz a taxa de aprendizado suavemente ao longo das √©pocas)\n"
      ],
      "metadata": {
        "id": "rwr-3FVAnk6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "def get_criterion(df):\n",
        "    # Calcula pos_weight = (negativos / positivos)\n",
        "    pos = (df[\"label\"] == 1).sum()\n",
        "    neg = (df[\"label\"] == 0).sum()\n",
        "    pos_weight = torch.tensor([neg / pos], device=device, dtype=torch.float32)\n",
        "\n",
        "    print(\"pos_weight:\", pos_weight.item())\n",
        "    return nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "criterion = get_criterion(df_train)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-15T21:13:59.723685Z",
          "iopub.execute_input": "2026-02-15T21:13:59.724494Z",
          "iopub.status.idle": "2026-02-15T21:13:59.736433Z",
          "shell.execute_reply.started": "2026-02-15T21:13:59.724441Z",
          "shell.execute_reply": "2026-02-15T21:13:59.735812Z"
        },
        "id": "AyXfYyLXnk6F"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3 Treino e Valida√ß√£o\n",
        "\n",
        "Durante o treino, o modelo recebe imagens e retorna logits.\n",
        "Na valida√ß√£o, convertemos logits em probabilidades usando `sigmoid` e calculamos ROC-AUC.\n",
        "\n",
        "Importante:\n",
        "- ROC-AUC usa **scores cont√≠nuos** (probabilidades), n√£o r√≥tulos 0/1.\n"
      ],
      "metadata": {
        "id": "tJTvUYbfnk6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device).unsqueeze(1)  # shape (B,1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = model(x)             # sa√≠da: logit\n",
        "        loss = criterion(logits, y)   # BCEWithLogitsLoss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * x.size(0)\n",
        "\n",
        "    return running_loss / len(loader.dataset)\n",
        "\n",
        "@torch.no_grad()\n",
        "def valid_one_epoch(model, loader):\n",
        "    model.eval()\n",
        "    probs = []\n",
        "    ys = []\n",
        "\n",
        "    for x, y in loader:\n",
        "        x = x.to(device)\n",
        "\n",
        "        logits = model(x)\n",
        "        p = torch.sigmoid(logits).cpu().numpy().ravel()  # probabilidade de pneumonia\n",
        "\n",
        "        probs.append(p)\n",
        "        ys.append(y.numpy().ravel())\n",
        "\n",
        "    probs = np.concatenate(probs)\n",
        "    ys = np.concatenate(ys)\n",
        "\n",
        "    auc = roc_auc_score(ys, probs)\n",
        "    return auc"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-15T21:13:59.739208Z",
          "iopub.execute_input": "2026-02-15T21:13:59.739440Z",
          "iopub.status.idle": "2026-02-15T21:13:59.746531Z",
          "shell.execute_reply.started": "2026-02-15T21:13:59.739409Z",
          "shell.execute_reply": "2026-02-15T21:13:59.745698Z"
        },
        "id": "-eZE8Ls-nk6F"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.4 Treinamento Inicial (Fold √∫nico)\n",
        "\n",
        "Primeiro treinamos em um √∫nico fold para validar o pipeline completo:\n",
        "\n",
        "- DataLoader\n",
        "- Modelo\n",
        "- Loss\n",
        "- AUC\n",
        "\n",
        "Ap√≥s confirmar que est√° funcionando, expandiremos para 5 folds e ensemble.\n"
      ],
      "metadata": {
        "id": "tqGx2N1znk6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configura√ß√£o inicial (Fold √∫nico) + salvar best_state\n",
        "\n",
        "FOLD = 0\n",
        "EPOCHS = 3\n",
        "LR = 3e-4\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_loader, valid_loader = make_loaders(df_train, fold=FOLD, batch_size=BATCH_SIZE)\n",
        "\n",
        "model = build_model().to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "\n",
        "best_auc = -1\n",
        "best_state = None  # garante que existe mesmo se algo der errado\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer)\n",
        "    val_auc = valid_one_epoch(model, valid_loader)\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"Epoch {epoch}/{EPOCHS} | loss={train_loss:.4f} | val_auc={val_auc:.4f}\")\n",
        "\n",
        "    # Salva o melhor checkpoint (na CPU para economizar VRAM e facilitar reutiliza√ß√£o)\n",
        "    if val_auc > best_auc:\n",
        "        best_auc = val_auc\n",
        "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "\n",
        "print(\"Best AUC:\", best_auc)\n",
        "\n",
        "# Recarrega o melhor estado no modelo (para garantir que o 'model' final √© o melhor)\n",
        "model.load_state_dict(best_state)\n",
        "model = model.to(device)\n",
        "print(\"‚úÖ Best model carregado no objeto 'model'.\")\n",
        "\n",
        "# (Opcional) Salvar em arquivo para usar depois (Se√ß√£o 7 / Grad-CAM) sem retreinar\n",
        "torch.save(best_state, f\"best_model_fold{FOLD}.pth\")\n",
        "print(f\"‚úÖ Checkpoint salvo em: best_model_fold{FOLD}.pth\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-15T21:13:59.747413Z",
          "iopub.execute_input": "2026-02-15T21:13:59.747663Z"
        },
        "id": "mPXITVdfnk6F"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.5 Treinamento com Valida√ß√£o Cruzada (5-Fold) e Ensemble\n",
        "\n",
        "Para aumentar robustez e reduzir vari√¢ncia do modelo, treinamos 1 modelo por fold.\n",
        "\n",
        "Para o conjunto de teste, geramos probabilidades com cada modelo e tiramos a m√©dia (ensemble),\n",
        "o que normalmente melhora a performance em competi√ß√µes Kaggle.\n",
        "\n",
        "Nesta etapa registramos:\n",
        "- AUC de cada fold\n",
        "- AUC m√©dio\n",
        "- Probabilidades finais no teste (m√©dia dos folds)\n"
      ],
      "metadata": {
        "id": "0qup_PT8nk6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def predict_proba(model, loader):\n",
        "    #Retorna probabilidades (sigmoid) para todos os itens do loader, na ordem.\n",
        "    model.eval()\n",
        "    all_probs = []\n",
        "    for batch in loader:\n",
        "        # batch pode ser (x,y) ou s√≥ x (teste)\n",
        "        if isinstance(batch, (list, tuple)) and len(batch) == 2:\n",
        "            x, _ = batch\n",
        "        else:\n",
        "            x = batch\n",
        "\n",
        "        x = x.to(device)\n",
        "        logits = model(x)\n",
        "        probs = torch.sigmoid(logits).detach().cpu().numpy().ravel()\n",
        "        all_probs.append(probs)\n",
        "\n",
        "    return np.concatenate(all_probs)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "-eUtfzI_nk6F"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "N_SPLITS = 5\n",
        "EPOCHS = 3\n",
        "LR = 3e-4\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "fold_aucs = []\n",
        "test_pred_folds = []\n",
        "\n",
        "best_states = []        # guarda best_state em mem√≥ria (opcional, mas √∫til)\n",
        "val_probs_folds = []    # (opcional) probs de valida√ß√£o por fold\n",
        "val_true_folds = []     # (opcional) labels verdadeiros por fold\n",
        "\n",
        "# dataset/loader de teste\n",
        "test_dataset = XRayDataset(df_test, transform=valid_tfms, has_label=False)\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=(device.type == \"cuda\")\n",
        ")\n",
        "\n",
        "for fold in range(N_SPLITS):\n",
        "    print(f\"\\n==================== FOLD {fold} ====================\")\n",
        "\n",
        "    # Loaders do fold atual\n",
        "    train_loader, valid_loader = make_loaders(\n",
        "        df_train, fold=fold, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    # Modelo + otimizador + scheduler\n",
        "    model = build_model().to(device)\n",
        "    optimizer = AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "\n",
        "    best_auc = -1\n",
        "    best_state = None\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        train_loss = train_one_epoch(model, train_loader, optimizer)\n",
        "        val_auc = valid_one_epoch(model, valid_loader)\n",
        "        scheduler.step()\n",
        "\n",
        "        print(f\"Epoch {epoch}/{EPOCHS} | loss={train_loss:.4f} | val_auc={val_auc:.4f}\")\n",
        "\n",
        "        if val_auc > best_auc:\n",
        "            best_auc = val_auc\n",
        "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "\n",
        "    # Guarda AUC do fold\n",
        "    fold_aucs.append(best_auc)\n",
        "    print(f\"Best AUC (fold {fold}): {best_auc:.6f}\")\n",
        "\n",
        "    # Salva best_state em mem√≥ria (opcional)\n",
        "    best_states.append(best_state)\n",
        "\n",
        "    # Salva best_state em arquivo (recomendado)\n",
        "    ckpt_path = f\"best_model_fold{fold}.pth\"\n",
        "    torch.save(best_state, ckpt_path)\n",
        "    print(f\"‚úÖ Checkpoint salvo em: {ckpt_path}\")\n",
        "\n",
        "    # Carrega melhor estado e prediz no teste\n",
        "    model.load_state_dict(best_state)\n",
        "    model = model.to(device)\n",
        "\n",
        "    test_probs = predict_proba(model, test_loader)\n",
        "    test_pred_folds.append(test_probs)\n",
        "\n",
        "    # (Opcional) salvar probs da valida√ß√£o (para Se√ß√£o 7 sem retrain)\n",
        "    df_va = df_train[df_train[\"fold\"] == fold].reset_index(drop=True)\n",
        "    val_probs = predict_proba(model, valid_loader)  # valid_loader j√° est√° no fold certo e sem shuffle\n",
        "    val_probs_folds.append(val_probs)\n",
        "    val_true_folds.append(df_va[\"label\"].values)\n",
        "\n",
        "# Resultado final\n",
        "print(\"\\nAUCs por fold:\", [round(float(a), 6) for a in fold_aucs])\n",
        "print(\"AUC m√©dio:\", float(np.mean(fold_aucs)))\n",
        "\n",
        "# Ensemble (m√©dia das probabilidades dos folds)\n",
        "test_pred_mean = np.mean(np.stack(test_pred_folds, axis=0), axis=0)\n",
        "print(\"test_pred_mean shape:\", test_pred_mean.shape)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "KmJwu3qCnk6F"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Avalia√ß√£o e An√°lise de Erros (Fold 0)\n",
        "\n",
        "A m√©trica principal da competi√ß√£o √© ROC-AUC, que mede a capacidade do modelo de ranquear corretamente\n",
        "exemplos positivos acima de negativos, sem exigir um limiar (threshold) fixo.\n",
        "\n",
        "Entretanto, para interpretar o comportamento do modelo em um cen√°rio de decis√£o bin√°ria\n",
        "(e para o relat√≥rio), tamb√©m avaliamos m√©tricas dependentes de threshold:\n",
        "\n",
        "- Matriz de confus√£o (TN, FP, FN, TP)\n",
        "- Precision, Recall, F1-score\n",
        "- An√°lise qualitativa de erros (FP e FN)\n",
        "\n",
        "**Observa√ß√£o importante:** escolher um threshold √© uma decis√£o de \"m√©trica de neg√≥cio\".\n",
        "Em aplica√ß√µes m√©dicas, geralmente buscamos **alto recall** (reduzir falsos negativos),\n",
        "mesmo que isso aumente falsos positivos.\n"
      ],
      "metadata": {
        "id": "4CVRzxBFnk6G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.1 Probabilidades na valida√ß√£o (Fold 0)\n",
        "\n",
        "Nesta etapa:\n",
        "1. Selecionamos o conjunto de valida√ß√£o do fold 0 (definido pelo StratifiedKFold).\n",
        "2. Carregamos o checkpoint salvo (`best_model_fold0.pth`).\n",
        "3. Geramos probabilidades para cada imagem do fold 0:\n",
        "\n",
        "- O modelo retorna **logits** (valores reais).\n",
        "- Aplicamos `sigmoid(logit)` para obter **probabilidade** de pneumonia.\n",
        "\n",
        "Essas probabilidades ser√£o usadas para ROC-AUC, matriz de confus√£o e an√°lise de erros.\n"
      ],
      "metadata": {
        "id": "JjY2A-6lnk6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7.1 Gerar probabilidades na valida√ß√£o (Fold 0)\n",
        "\n",
        "FOLD_ANALYSIS = 0\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "# 1) Filtra os dados do fold 0 (valida√ß√£o)\n",
        "df_va = df_train[df_train[\"fold\"] == FOLD_ANALYSIS].reset_index(drop=True)\n",
        "\n",
        "# 2) DataLoader da valida√ß√£o (sem shuffle para manter a ordem)\n",
        "val_dataset = XRayDataset(df_va, transform=valid_tfms, has_label=True)\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=(device.type == \"cuda\")\n",
        ")\n",
        "\n",
        "# 3) Carrega checkpoint salvo do melhor modelo (fold 0)\n",
        "state = torch.load(\"best_model_fold0.pth\", map_location=\"cpu\")\n",
        "\n",
        "model = build_model().to(device)\n",
        "model.load_state_dict(state)\n",
        "model.eval()\n",
        "\n",
        "# 4) Gera probabilidades p(y=1) usando sigmoid(logits)\n",
        "val_probs = predict_proba(model, val_loader)  # shape (N,)\n",
        "val_true  = df_va[\"label\"].values             # shape (N,)\n",
        "\n",
        "print(\"val_probs shape:\", val_probs.shape)\n",
        "print(\"ROC-AUC (fold 0):\", roc_auc_score(val_true, val_probs))\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "DcP6Dgyknk6G"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.2 M√©tricas com threshold (0.5)\n",
        "\n",
        "Como baseline, usamos threshold = 0.5:\n",
        "\n",
        "- Se `p >= 0.5`, o modelo prediz pneumonia (classe 1).\n",
        "- Caso contr√°rio, prediz normal (classe 0).\n",
        "\n",
        "A matriz de confus√£o segue o padr√£o:\n",
        "\n",
        "[[TN, FP],\n",
        " [FN, TP]]\n",
        "\n",
        "Onde:\n",
        "- **FN** (falso negativo) √© o caso mais cr√≠tico: pneumonia predita como normal.\n",
        "- **FP** (falso positivo) pode gerar alarme indevido, mas √© menos cr√≠tico em triagem.\n"
      ],
      "metadata": {
        "id": "_nOGMRAnnk6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7.2 Matriz de confus√£o + relat√≥rio (threshold=0.5)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "thr = 0.5\n",
        "\n",
        "# Converte probabilidade em predi√ß√£o bin√°ria\n",
        "val_pred = (val_probs >= thr).astype(int)\n",
        "\n",
        "# Matriz de confus√£o\n",
        "cm = confusion_matrix(val_true, val_pred)\n",
        "print(\"Confusion Matrix [[TN, FP],[FN, TP]]:\\n\", cm)\n",
        "\n",
        "# Relat√≥rio completo: precision/recall/f1 por classe\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(val_true, val_pred, digits=4))\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "XRMXOyfSnk6G"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.3 Ajuste de threshold focando recall (m√©trica de neg√≥cio)\n",
        "\n",
        "Como ROC-AUC √© independente de threshold, a escolha de limiar √© uma decis√£o do sistema.\n",
        "\n",
        "Aqui testamos v√°rios thresholds e escolhemos aquele que maximiza **recall** na valida√ß√£o.\n",
        "Isso tende a reduzir FN (mais seguran√ßa), por√©m aumenta FP.\n"
      ],
      "metadata": {
        "id": "Bp86Oyipnk6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7.3 Buscar threshold que maximize recall\n",
        "\n",
        "thresholds = np.linspace(0.05, 0.95, 19)\n",
        "\n",
        "best_thr = 0.5\n",
        "best_recall = -1\n",
        "best_tuple = None  # (tn, fp, fn, tp)\n",
        "\n",
        "for t in thresholds:\n",
        "    pred = (val_probs >= t).astype(int)\n",
        "    tn, fp, fn, tp = confusion_matrix(val_true, pred).ravel()\n",
        "\n",
        "    # Recall = TP / (TP + FN)\n",
        "    recall = tp / (tp + fn + 1e-9)\n",
        "\n",
        "    if recall > best_recall:\n",
        "        best_recall = recall\n",
        "        best_thr = t\n",
        "        best_tuple = (tn, fp, fn, tp)\n",
        "\n",
        "tn, fp, fn, tp = best_tuple\n",
        "print(\"Melhor threshold (max recall):\", best_thr)\n",
        "print(f\"TN={tn} FP={fp} FN={fn} TP={tp}\")\n",
        "print(\"Recall nesse threshold:\", best_recall)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "D5KmBFdKnk6G"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.4 Inspe√ß√£o visual de erros (FP e FN)\n",
        "\n",
        "Para interpretar os erros, exibimos imagens classificadas incorretamente:\n",
        "\n",
        "- **Falsos Positivos (FP):** normal predito como pneumonia\n",
        "- **Falsos Negativos (FN):** pneumonia predito como normal (mais cr√≠tico)\n",
        "\n",
        "Nos gr√°ficos, mostramos `p=...` acima de cada imagem:\n",
        "\n",
        "- `p` √© a probabilidade prevista de pneumonia (`target`).\n",
        "- Ex.: `p=0.62` significa que o modelo estimou 62% de chance de pneumonia.\n",
        "\n",
        "A inspe√ß√£o visual ajuda a entender:\n",
        "- padr√µes de erro associados a contraste/exposi√ß√£o do raio X\n",
        "- casos amb√≠guos e ‚Äúdif√≠ceis‚Äù\n",
        "- poss√≠veis atalhos (spurious correlations) que o modelo pode estar usando\n"
      ],
      "metadata": {
        "id": "hvgkMAVbnk6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7.4 Mostrar exemplos de FP/FN para an√°lise de erro\n",
        "\n",
        "def show_examples(df_va, y_true, y_prob, kind=\"FN\", thr=0.5, n=6):\n",
        "\n",
        "    #Exibe exemplos de erros com a probabilidade prevista p(y=1) no t√≠tulo.\n",
        "\n",
        "    #kind=\"FN\": y=1 e pred=0\n",
        "    #kind=\"FP\": y=0 e pred=1\n",
        "\n",
        "    y_pred = (y_prob >= thr).astype(int)\n",
        "\n",
        "    if kind == \"FN\":\n",
        "        idx = np.where((y_true == 1) & (y_pred == 0))[0]\n",
        "        title = \"Falsos Negativos (Pneumonia ‚Üí Normal)\"\n",
        "    else:\n",
        "        idx = np.where((y_true == 0) & (y_pred == 1))[0]\n",
        "        title = \"Falsos Positivos (Normal ‚Üí Pneumonia)\"\n",
        "\n",
        "    if len(idx) == 0:\n",
        "        print(f\"Nenhum exemplo de {kind} encontrado nesse threshold.\")\n",
        "        return\n",
        "\n",
        "    # Seleciona os exemplos mais pr√≥ximos do threshold (casos mais amb√≠guos)\n",
        "    idx = sorted(idx, key=lambda i: abs(y_prob[i] - thr))[:n]\n",
        "\n",
        "    plt.figure(figsize=(3*n, 3))\n",
        "    for j, k in enumerate(idx):\n",
        "        path = df_va.loc[k, \"path\"]\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "\n",
        "        plt.subplot(1, len(idx), j+1)\n",
        "        plt.imshow(img)\n",
        "        plt.title(f\"p={y_prob[k]:.2f}\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    plt.suptitle(title)\n",
        "    plt.show()\n",
        "\n",
        "# Use thr=0.5 para a an√°lise padr√£o\n",
        "show_examples(df_va, val_true, val_probs, kind=\"FP\", thr=0.5, n=6)\n",
        "show_examples(df_va, val_true, val_probs, kind=\"FN\", thr=0.5, n=6)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "fLJ_yoFWnk6H"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Interpretabilidade (XAI) com Grad-CAM\n",
        "\n",
        "Para entender melhor como o modelo toma decis√µes, aplicamos Grad-CAM (Gradient-weighted Class Activation Mapping).\n",
        "\n",
        "O Grad-CAM produz um mapa de calor que indica as regi√µes da imagem que mais contribu√≠ram para a predi√ß√£o.\n",
        "Isso √© √∫til para:\n",
        "\n",
        "- validar se o modelo est√° focando em regi√µes clinicamente plaus√≠veis (ex.: √°reas pulmonares);\n",
        "- investigar erros (FP e FN) e poss√≠veis \"atalhos\" (artefatos, bordas, textos, contraste);\n",
        "- enriquecer o relat√≥rio com interpretabilidade, conectando m√©tricas com explica√ß√µes visuais.\n",
        "\n",
        "Nesta se√ß√£o:\n",
        "- carregamos o melhor modelo do fold 0 (checkpoint salvo);\n",
        "- selecionamos exemplos (TP, FP, FN) do fold 0;\n",
        "- geramos mapas Grad-CAM e sobrepomos na imagem original.\n"
      ],
      "metadata": {
        "id": "y8eUSCJjnk6H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.1 Carregar modelo (Fold 0)\n",
        "\n",
        "Usaremos o mesmo checkpoint utilizado na Se√ß√£o 7 (`best_model_fold0.pth`) para garantir consist√™ncia:\n",
        "as explica√ß√µes (Grad-CAM) refletem exatamente o modelo avaliado.\n"
      ],
      "metadata": {
        "id": "YwI_eaRNnk6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega o checkpoint do fold 0\n",
        "state = torch.load(\"best_model_fold0.pth\", map_location=\"cpu\")\n",
        "\n",
        "model = build_model().to(device)\n",
        "model.load_state_dict(state)\n",
        "model.eval();\n",
        "\n",
        "print(\"‚úÖ Modelo do fold 0 carregado para Grad-CAM.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "X4TjdWR3nk6H"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.2 Implementa√ß√£o do Grad-CAM\n",
        "\n",
        "O Grad-CAM usa:\n",
        "- as ativa√ß√µes de uma camada convolucional (feature maps)\n",
        "- os gradientes da sa√≠da (classe positiva) em rela√ß√£o a essas ativa√ß√µes\n",
        "\n",
        "Em seguida, calcula um mapa de calor 2D (H√óW) que indica a contribui√ß√£o de cada regi√£o.\n",
        "\n",
        "Escolhemos como camada alvo a √∫ltima camada convolucional da EfficientNet (√∫ltimo bloco em `model.features`),\n",
        "pois ela costuma capturar padr√µes de alto n√≠vel relevantes para a decis√£o.\n"
      ],
      "metadata": {
        "id": "Wa6Un2Vank6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GradCAM:\n",
        "\n",
        "    # Implementa√ß√£o simples de Grad-CAM para um modelo PyTorch.\n",
        "    # Funciona bem para arquiteturas conv (como EfficientNet/ResNet).\n",
        "\n",
        "    def __init__(self, model, target_layer):\n",
        "        self.model = model\n",
        "        self.target_layer = target_layer\n",
        "\n",
        "        self.activations = None\n",
        "        self.gradients = None\n",
        "\n",
        "        # Hook: captura ativa√ß√µes no forward\n",
        "        def forward_hook(module, inp, out):\n",
        "            self.activations = out.detach()\n",
        "\n",
        "        # Hook: captura gradientes no backward\n",
        "        def backward_hook(module, grad_in, grad_out):\n",
        "            # grad_out √© uma tupla; grad_out[0] √© o gradiente das ativa√ß√µes\n",
        "            self.gradients = grad_out[0].detach()\n",
        "\n",
        "        self.fwd_handle = self.target_layer.register_forward_hook(forward_hook)\n",
        "        self.bwd_handle = self.target_layer.register_full_backward_hook(backward_hook)\n",
        "\n",
        "    def remove_hooks(self):\n",
        "        self.fwd_handle.remove()\n",
        "        self.bwd_handle.remove()\n",
        "\n",
        "    def __call__(self, x):\n",
        "\n",
        "        # x: tensor (1, C, H, W) j√° transformado/normalizado.\n",
        "        # Retorna:\n",
        "        # - cam: heatmap 2D normalizado [0,1] (H, W)\n",
        "        # - prob: probabilidade prevista p(y=1)\n",
        "\n",
        "        self.model.zero_grad()\n",
        "\n",
        "        # Forward: logits\n",
        "        logits = self.model(x)                  # shape (1,1)\n",
        "        prob = torch.sigmoid(logits).item()\n",
        "\n",
        "        # Backward: gradiente do logit da classe positiva (pneumonia)\n",
        "        logits.backward(torch.ones_like(logits))\n",
        "\n",
        "        # Ativa√ß√µes e gradientes: (1, C, h, w)\n",
        "        A = self.activations\n",
        "        dA = self.gradients\n",
        "\n",
        "        # Pesos = m√©dia global dos gradientes por canal: (C,)\n",
        "        weights = dA.mean(dim=(2, 3), keepdim=True)  # (1, C, 1, 1)\n",
        "\n",
        "        # Combina√ß√£o ponderada dos mapas: (1, 1, h, w)\n",
        "        cam = (weights * A).sum(dim=1, keepdim=True)\n",
        "\n",
        "        # ReLU (mant√©m apenas contribui√ß√µes positivas)\n",
        "        cam = torch.relu(cam)\n",
        "\n",
        "        # Normaliza para [0,1]\n",
        "        cam = cam.squeeze().cpu().numpy()\n",
        "        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-9)\n",
        "\n",
        "        return cam, prob\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "RgPL_Ubqnk6H"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.3 Visualiza√ß√£o\n",
        "\n",
        "Para visualizar o Grad-CAM:\n",
        "- carregamos a imagem original (para exibi√ß√£o)\n",
        "- aplicamos as transforma√ß√µes de valida√ß√£o (`valid_tfms`) para alimentar o modelo\n",
        "- geramos o mapa Grad-CAM em baixa resolu√ß√£o e fazemos resize para o tamanho da imagem exibida\n",
        "- sobrepomos o heatmap na imagem original\n"
      ],
      "metadata": {
        "id": "4qg6tqpInk6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "def load_image_rgb(path):\n",
        "    # Carrega imagem como RGB (PIL).\n",
        "    return Image.open(path).convert(\"RGB\")\n",
        "\n",
        "def tensor_from_path(path):\n",
        "    # Aplica valid_tfms e retorna tensor (1,C,H,W).\n",
        "    img = load_image_rgb(path)\n",
        "    x = valid_tfms(img).unsqueeze(0).to(device)\n",
        "    return x\n",
        "\n",
        "def overlay_heatmap_on_image(img_pil, cam, alpha=0.4):\n",
        "\n",
        "    # Sobrep√µe o heatmap (cam) em uma imagem PIL.\n",
        "    # cam: array 2D em [0,1]\n",
        "\n",
        "    img = np.array(img_pil)  # (H,W,3) RGB\n",
        "    H, W = img.shape[:2]\n",
        "\n",
        "    # Redimensiona CAM para o tamanho da imagem\n",
        "    cam_resized = cv2.resize(cam, (W, H))\n",
        "\n",
        "    # Converte cam em heatmap colorido (colormap)\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * cam_resized), cv2.COLORMAP_JET)\n",
        "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Combina imagem + heatmap\n",
        "    overlay = (1 - alpha) * img + alpha * heatmap\n",
        "    overlay = np.clip(overlay, 0, 255).astype(np.uint8)\n",
        "\n",
        "    return overlay\n",
        "\n",
        "def show_gradcam(path, cam, prob, title=\"\"):\n",
        "    # Exibe imagem original e imagem com overlay do Grad-CAM.\n",
        "    img = load_image_rgb(path)\n",
        "    overlay = overlay_heatmap_on_image(img, cam, alpha=0.4)\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(img)\n",
        "    plt.title(\"Original\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(overlay)\n",
        "    plt.title(f\"{title}\\n p(pneumonia)={prob:.3f}\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "ZZ1N6Qaknk6H"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.4 Sele√ß√£o de exemplos para an√°lise (TP / FP / FN)\n",
        "\n",
        "Usamos as probabilidades da valida√ß√£o (Se√ß√£o 7) para selecionar exemplos representativos:\n",
        "- **TP**: pneumonia correta com alta confian√ßa\n",
        "- **FP**: normal predito como pneumonia (erro)\n",
        "- **FN**: pneumonia predito como normal (erro mais cr√≠tico)\n",
        "\n",
        "Isso facilita escolher casos informativos sem sele√ß√£o manual.\n"
      ],
      "metadata": {
        "id": "CGgmImw3nk6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8.4 Seleciona √≠ndices de TP/FP/FN com threshold 0.5\n",
        "\n",
        "thr = 0.5\n",
        "val_pred = (val_probs >= thr).astype(int)\n",
        "\n",
        "# √≠ndices por tipo\n",
        "tp_idx = np.where((val_true == 1) & (val_pred == 1))[0]\n",
        "fp_idx = np.where((val_true == 0) & (val_pred == 1))[0]\n",
        "fn_idx = np.where((val_true == 1) & (val_pred == 0))[0]\n",
        "\n",
        "print(\"TP:\", len(tp_idx), \"FP:\", len(fp_idx), \"FN:\", len(fn_idx))\n",
        "\n",
        "# Escolhas \"informativas\":\n",
        "# - TP mais confiante: maior prob\n",
        "tp_best = tp_idx[np.argmax(val_probs[tp_idx])] if len(tp_idx) else None\n",
        "\n",
        "# - FP mais confiante: maior prob (modelo muito certo e mesmo assim errou)\n",
        "fp_best = fp_idx[np.argmax(val_probs[fp_idx])] if len(fp_idx) else None\n",
        "\n",
        "# - FN mais \"perigoso\": menor prob (modelo muito certo que √© normal, mas era pneumonia)\n",
        "fn_best = fn_idx[np.argmin(val_probs[fn_idx])] if len(fn_idx) else None\n",
        "\n",
        "tp_best, fp_best, fn_best\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "OW4erl34nk6H"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.5 Grad-CAM nos exemplos selecionados\n",
        "\n",
        "Geramos o Grad-CAM para cada caso e verificamos se as regi√µes destacadas fazem sentido.\n",
        "\n",
        "Idealmente:\n",
        "- casos de pneumonia: o modelo deve destacar regi√µes pulmonares com opacidades/infiltrados;\n",
        "- casos normais: o foco n√£o deveria estar em bordas, marcas ou regi√µes fora do t√≥rax.\n",
        "\n",
        "Em erros (FP/FN), analisamos se o modelo foi \"enganado\" por contraste, exposi√ß√£o ou artefatos.\n"
      ],
      "metadata": {
        "id": "5hq2Ux8Unk6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8.5 Executar Grad-CAM\n",
        "\n",
        "# camada alvo: √∫ltimo bloco convolucional da EfficientNet\n",
        "target_layer = model.features[-1]\n",
        "\n",
        "gradcam = GradCAM(model, target_layer)\n",
        "\n",
        "def run_one(idx, label_name):\n",
        "    if idx is None:\n",
        "        print(f\"Sem exemplo dispon√≠vel para {label_name}.\")\n",
        "        return\n",
        "    path = df_va.loc[idx, \"path\"]\n",
        "    x = tensor_from_path(path)\n",
        "\n",
        "    # Ativa gradiente (necess√°rio para Grad-CAM)\n",
        "    model.zero_grad()\n",
        "    cam, prob = gradcam(x)\n",
        "\n",
        "    # t√≠tulo: tipo + classe verdadeira\n",
        "    y_true = int(df_va.loc[idx, \"label\"])\n",
        "    title = f\"{label_name} | y_true={y_true}\"\n",
        "    show_gradcam(path, cam, prob, title=title)\n",
        "\n",
        "run_one(tp_best, \"TP (acerto pneumonia)\")\n",
        "run_one(fp_best, \"FP (normal‚Üípneumonia)\")\n",
        "run_one(fn_best, \"FN (pneumonia‚Üínormal)\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "3mHcqglnnk6H"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.6 Interpreta√ß√£o dos mapas Grad-CAM (TP / FP / FN)\n",
        "\n",
        "A an√°lise por Grad-CAM permite verificar se o modelo toma decis√µes baseadas em regi√µes\n",
        "anatomicamente plaus√≠veis (ex.: campos pulmonares) ou se utiliza \"atalhos\" (artefatos e bordas).\n",
        "\n",
        "### Caso TP (acerto ‚Äì pneumonia)\n",
        "No exemplo verdadeiro positivo (TP), o mapa Grad-CAM se concentrou predominantemente nos pulm√µes.\n",
        "Esse padr√£o √© consistente com a tarefa, j√° que altera√ß√µes compat√≠veis com pneumonia em radiografias\n",
        "tendem a se manifestar como opacidades/infiltrados nos campos pulmonares.  \n",
        "**Interpreta√ß√£o:** o modelo utiliza evid√™ncias visuais relevantes para a decis√£o, indicando boa plausibilidade cl√≠nica.\n",
        "\n",
        "### Caso FP (erro ‚Äì normal ‚Üí pneumonia)\n",
        "No falso positivo (FP), observou-se ativa√ß√£o forte na borda direita da imagem, em uma regi√£o escura\n",
        "fora da √°rea de interesse (fora do t√≥rax).  \n",
        "**Interpreta√ß√£o:** o modelo possivelmente foi influenciado por um artefato/estrutura de borda,\n",
        "em vez de padr√µes internos do par√™nquima pulmonar. Esse comportamento √© t√≠pico quando o modelo aprende\n",
        "pistas n√£o relacionadas √† patologia (ex.: moldura do exame, contraste, ru√≠do, cortes da imagem).\n",
        "\n",
        "### Caso FN (erro mais cr√≠tico ‚Äì pneumonia ‚Üí normal)\n",
        "No falso negativo (FN), o Grad-CAM focou em uma regi√£o perif√©rica pr√≥xima a estruturas √≥sseas (√°rea mais branca),\n",
        "em vez de destacar de forma predominante os campos pulmonares.  \n",
        "**Interpreta√ß√£o:** o modelo pode ter supervalorizado padr√µes de alto contraste (ossos/exposi√ß√£o) e subestimado sinais\n",
        "pulmonares mais sutis, resultando na n√£o detec√ß√£o do caso positivo. Em um cen√°rio cl√≠nico, esse tipo de erro √© mais grave,\n",
        "pois representa pneumonia n√£o sinalizada pelo sistema.\n",
        "\n",
        "### Implica√ß√µes e poss√≠veis melhorias\n",
        "Os casos FP/FN sugerem que, apesar do alto desempenho global, o modelo pode ocasionalmente se apoiar em regi√µes\n",
        "fora do pulm√£o. Estrat√©gias que podem mitigar esse efeito incluem:\n",
        "- pr√©-processamento para reduzir influ√™ncia de bordas (ex.: crop central ou remo√ß√£o de margens);\n",
        "- augmentations mais robustas (varia√ß√£o de contraste/brightness) para reduzir depend√™ncia de exposi√ß√£o;\n",
        "- (opcional) uso de segmenta√ß√£o/ROI dos pulm√µes para for√ßar o foco na √°rea de interesse.\n"
      ],
      "metadata": {
        "id": "v-uuQ7uXnk6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"gradcam_outputs\", exist_ok=True)\n",
        "\n",
        "def save_gradcam_figure(path, cam, prob, title, out_name):\n",
        "    img = load_image_rgb(path)\n",
        "    overlay = overlay_heatmap_on_image(img, cam, alpha=0.4)\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(img)\n",
        "    plt.title(\"Original\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(overlay)\n",
        "    plt.title(f\"{title}\\n p(pneumonia)={prob:.3f}\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    out_path = os.path.join(\"gradcam_outputs\", out_name)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path, dpi=200, bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "    print(\"Salvo em:\", out_path)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-15T21:15:08.078714Z",
          "iopub.execute_input": "2026-02-15T21:15:08.079199Z",
          "iopub.status.idle": "2026-02-15T21:15:08.084880Z",
          "shell.execute_reply.started": "2026-02-15T21:15:08.079170Z",
          "shell.execute_reply": "2026-02-15T21:15:08.083936Z"
        },
        "id": "N0bV8Wdpnk6H"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Gera√ß√£o do Arquivo de Submiss√£o\n",
        "\n",
        "Nesta se√ß√£o geramos o arquivo `submission.csv` no formato exigido pela competi√ß√£o.\n",
        "\n",
        "A predi√ß√£o final do conjunto de teste utiliza **ensemble dos 5 folds**, calculando a m√©dia das probabilidades:\n",
        "\n",
        "- `id`: identificador do arquivo (exatamente como em `test.csv`)\n",
        "- `target`: probabilidade prevista para pneumonia (classe positiva)\n",
        "\n",
        "Ao final, salvamos `submission.csv` no diret√≥rio de trabalho para envio no Kaggle.\n"
      ],
      "metadata": {
        "id": "VjKwXLdhnk6I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.1 Constru√ß√£o do arquivo `submission.csv` (com sanity checks)\n",
        "\n",
        "Antes de salvar, fazemos verifica√ß√µes r√°pidas para evitar erros comuns:\n",
        "\n",
        "- O n√∫mero de linhas deve ser igual ao n√∫mero de amostras no teste.\n",
        "- Os `id`s n√£o devem ter duplicatas.\n",
        "- Os valores de `target` devem estar no intervalo [0, 1].\n"
      ],
      "metadata": {
        "id": "jorNt-ZVnk6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 9.1 Gerar submission.csv e salvar tamb√©m no Drive\n",
        "\n",
        "# Garante que temos predi√ß√µes para todo o teste\n",
        "assert \"test_pred_mean\" in globals(), \"test_pred_mean n√£o encontrado. Rode a etapa de ensemble (Se√ß√£o 6.5).\"\n",
        "assert \"df_test\" in globals(), \"df_test n√£o encontrado. Rode a Se√ß√£o 4.3.\"\n",
        "assert len(test_pred_mean) == len(df_test), \"Quantidade de predi√ß√µes diferente do tamanho do df_test.\"\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    \"id\": df_test[\"id\"],                      # ids oficiais do test.csv\n",
        "    \"target\": test_pred_mean.astype(float)    # probabilidades (ensemble)\n",
        "})\n",
        "\n",
        "# Sanity checks\n",
        "assert submission[\"id\"].nunique() == len(submission), \"IDs duplicados na submiss√£o.\"\n",
        "assert submission[\"target\"].between(0, 1).all(), \"Existem valores de target fora de [0, 1].\"\n",
        "\n",
        "# 1) Salva no diret√≥rio atual do runtime (Colab: /content)\n",
        "local_path = os.path.abspath(\"submission.csv\")\n",
        "submission.to_csv(local_path, index=False)\n",
        "\n",
        "# 2) Salva tamb√©m no Drive (persistente)\n",
        "assert \"PROJECT_DIR\" in globals(), \"PROJECT_DIR n√£o definido. Defina-o na Se√ß√£o 0.5 (Drive).\"\n",
        "drive_path = os.path.join(PROJECT_DIR, \"submission.csv\")\n",
        "submission.to_csv(drive_path, index=False)\n",
        "\n",
        "print(\"‚úÖ submission.csv salvo com sucesso!\")\n",
        "print(\"Local:\", local_path)\n",
        "print(\"Drive:\", drive_path)\n",
        "\n",
        "display(submission.head())\n",
        "print(\"Shape:\", submission.shape)\n",
        "print(\"Target min/mean/max:\",\n",
        "      submission[\"target\"].min(),\n",
        "      submission[\"target\"].mean(),\n",
        "      submission[\"target\"].max())\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-15T21:17:05.364228Z",
          "iopub.execute_input": "2026-02-15T21:17:05.364873Z",
          "iopub.status.idle": "2026-02-15T21:17:05.386357Z",
          "shell.execute_reply.started": "2026-02-15T21:17:05.364845Z",
          "shell.execute_reply": "2026-02-15T21:17:05.385691Z"
        },
        "id": "gVTstAWrnk6I"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}